**Updates coming stay tuned**

# ComfyUI LTX-2 VRAM Memory Management

**Generate extremely long videos with LTX-2 on consumer GPUs!**

This custom node dramatically reduces VRAM usage for LTX-2 video generation in ComfyUI, enabling 800-900+ (at 1920x1088) frame videos on a single 24GB GPU.

## üé¨ What This Does

LTX-2's FeedForward layers create massive intermediate tensors that normally limit video length. This node chunks those operations to reduce peak memory by up to **8x**, without any quality loss.

 | With V3 (ffn_chunks=16) |
|-------------------------|
| **900 frames @ 1920x1088** |
| Smooth generation |
| Single GPU works! |

## üìä Benchmarks (RTX 4090 24GB Single GPU Version (V3))

| Resolution | Frames | VRAM Used | Time |
|------------|--------|-----------|------|
| 1920√ó1088 | 800 | ~16.5 GB | ~15 min |
| 1920√ó1088 | 900 | ~18.5 GB | ~27 min |

*Results may vary based on your system configuration*


## üöÄ Installation

Copy this repo, and move the "comfyui_tensor_parallel_v2" and "comfyui_tensor_parallel_v3" folders to the customs_nodes folder

<img width="276" height="136" alt="image" src="https://github.com/user-attachments/assets/c8bcb9ae-505c-4e1c-93f6-9052a01a043d" />

Restart everything.

## üì¶ Included Nodes

### V3 - Single GPU (Recommended)
**Node name:** `Tensor Parallel V3 (Safe FFN Chunking)`

<img width="766" height="413" alt="image" src="https://github.com/user-attachments/assets/d7a6690a-612d-4239-a856-0dc2a5b18816" />

Best for most users. Works reliably with ComfyUI's memory management.

**Parameters:**
- `ffn_chunks`: Number of chunks (default: 8, recommended: **16** for long videos)
  - Higher = less VRAM but slightly slower
  - 16 chunks works well for 800-900 frames

**Usage:**
```
[Load Model] ‚Üí [Tensor Parallel V3] ‚Üí [Rest of workflow...]
```

### V2 - Multi-GPU (Experimental)
**Node name:** `Tensor Parallel V2 + Chunked FFN`

<img width="780" height="415" alt="image" src="https://github.com/user-attachments/assets/d906d458-eb63-4371-bee3-b19a8b044da9" />

For users with multiple GPUs who want faster generation. Distributes attention computation across GPUs.

**Parameters:**
- `num_gpus`: Number of GPUs to use
- `ffn_chunks`: FFN chunking (same as V3)
- `primary_gpu`: Main GPU for coordination

**Notes:**
- Faster than V3 but memory distribution is uneven
- GPU 0 uses more VRAM than others
- May require experimentation with settings

## üí° Tips

1. **Start with V3** - It's more stable and works great for most use cases

2. **Increase ffn_chunks for longer videos:**
   - 600 frames: `ffn_chunks=8`
   - 800 frames: `ffn_chunks=12-16`
   - 900+ frames: `ffn_chunks=16-24`

3. **If you get OOM errors:**
   - Increase `ffn_chunks`
   - Reduce resolution slightly
   - Close other GPU applications

4. **For V2 multi-GPU:**
   - Disable ComfyUI's lowvram/offloading mode
   - Expect GPU 0 to use more memory than others

## üîß Compatibility

- **Tested on:** Ubuntu with ComfyUI (official installation)
- **GPU:** NVIDIA RTX 4090 (24GB)
- **Model:** LTX-2 (LTXAV)

Other configurations may work but are untested. Please open an issue if you encounter problems!

## ü§î How It Works

LTX-2's FeedForward (FFN) layers expand the hidden dimension by 4x:
```
Input: (batch, 57000, 4096)      ~0.9 GB
  ‚Üì
Intermediate: (batch, 57000, 16384)  ~3.7 GB  ‚Üê Memory bottleneck!
  ‚Üì
Output: (batch, 57000, 4096)     ~0.9 GB
```

With 96 FFN layers in LTX-2, this adds up fast!

**Our solution:** Process the sequence in chunks:
```
Chunk 1: (batch, 7125, 4096) ‚Üí (batch, 7125, 16384) ‚Üí output
Chunk 2: (batch, 7125, 4096) ‚Üí (batch, 7125, 16384) ‚Üí output
... (8 chunks total)
Concatenate ‚Üí Full output

Peak memory: ~0.46 GB instead of ~3.7 GB per layer!
```

### V5 - Multi-GPU Sequence Parallelism with Ring Attention Made for I2V and T2V (NEW! You need a lot of VRAM)
**Node name:** `Tensor Parallel V5 (Sequence Parallel + Ring Attention)`
<img width="676" height="292" alt="image" src="https://github.com/user-attachments/assets/1d19d751-4473-45d1-bad8-7d37dfa33aa0" />


The most advanced option for users with multiple GPUs. Splits the **sequence dimension** across GPUs using ring attention, enabling extremely long videos that would be impossible on a single GPU.

**Key Innovation:** Instead of each GPU holding the full 600K+ token sequence, each GPU holds only 1/N of the tokens. Ring attention allows full self-attention computation by rotating K,V chunks around the ring.

**Parameters:**
- `num_gpus`: Number of GPUs to use (GPU 0 is excluded from sequence chunks to hold VAE/weights)
- `verbose`: Logging level (0=quiet, 1=info, 2=debug)

**Memory Distribution:**
```
Traditional: Each GPU needs full Q, K, V ‚Üí OOM at ~400K tokens
Ring Attention: Each GPU holds 1/N of sequence ‚Üí 600K+ tokens possible!

With 7 GPUs (6 active for sequence):
  - Total sequence: ~614,400 tokens
  - Per GPU: ~102,400 tokens
  - Peak VRAM per GPU: ~15-18GB
```

**How Ring Attention Works:**
1. Split sequence across N GPUs: GPU i holds tokens [i√óchunk : (i+1)√óchunk]
2. Each GPU computes Q from its local tokens
3. K,V chunks rotate around the ring (N iterations)
4. Online softmax accumulates attention without materializing full attention matrix
5. Result: Full self-attention with 1/N memory per GPU!

**Usage:**
```
[Load Model] ‚Üí [Tensor Parallel V5] ‚Üí [Rest of workflow...]
```

**Notes:**
- Best for Stage 2 upscaling with massive token counts
- GPU 0 is automatically excluded from sequence distribution (holds VAE)
- Uses float32 accumulators for numerical precision
- Properly handles both SPLIT and INTERLEAVED RoPE modes

**Requirements:**
- Multiple NVIDIA GPUs (tested with 7√ó RTX 4090)
- Works with LTXAV (audio-video) model

*Ring attention enables sequences that would OOM on any single GPU!*


## üôè Credits

- **Implementation:** Claude (Anthropic) + RandomInternetPreson
- **Tensor Parallelism Concepts (V2):** Inspired by [NVIDIA Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
- **Testing & Development:** RandomInternetPreson ([@RandomInternetPreson](https://github.com/RandomInternetPreson))
- **Workflow:** @yanokusnir Got good work flow from here: https://www.reddit.com/r/StableDiffusion/comments/1qae922/ltx2_i2v_isnt_perfect_but_its_still_awesome_my/
## üìù License

MIT License - Feel free to use, modify, and distribute!

## üêõ Issues & Contributions

Found a bug or have an improvement? Please open an issue or PR, I only have this running on Ubuntu so I don't know how much progress I can make on any Windows issues.

This is experimental software - your feedback helps make it better for everyone.

---

*Making long-form AI video accessible to everyone* üé•
